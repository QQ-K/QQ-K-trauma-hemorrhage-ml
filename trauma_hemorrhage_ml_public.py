# -*- coding: utf-8 -*-
"""trauma_hemorrhage_ML_public.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zzdGubv6kW7au-EsgRFf36SUvoJHXZ7e

# GitHub公開データ
"""

"""
Reproduces the main analyses in Kawai et al.
Policy: the operating threshold is pre-specified on OOF to keep FNR below five percent
and is applied unchanged to the independent test set.
"""

# ===== Imports =====
import numpy as np
import pandas as pd
import lightgbm as lgb
import optuna

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    roc_auc_score, roc_curve, confusion_matrix, brier_score_loss, auc
)
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
from scipy.stats import norm
from collections import Counter
from sklearn.model_selection import StratifiedShuffleSplit

# display fallback for non-notebook environments
try:
    from IPython.display import display
except Exception:
    def display(obj):
        if isinstance(obj, pd.DataFrame):
            print(obj.to_string(index=False))
        else:
            print(str(obj))

# ===== Global settings =====
SEED = 123
ES_ROUNDS = 200
np.random.seed(SEED)

# Threshold policy (kept fixed as in the manuscript)
USE_FIXED_THRESHOLD = True
FIXED_THRESHOLD     = 0.091

# ===== Utilities =====
def stratified_bootstrap_indices(y, n_boot=5000, seed=SEED, rng=None):
    """Class-stratified bootstrap indices. Accepts either a seed or a numpy Generator."""
    if rng is None:
        rng = np.random.default_rng(seed)
    y = np.asarray(y)
    pos = np.where(y == 1)[0]
    neg = np.where(y == 0)[0]
    out = []
    for _ in range(n_boot):
        bs_pos = rng.choice(pos, size=len(pos), replace=True)
        bs_neg = rng.choice(neg, size=len(neg), replace=True)
        out.append(np.concatenate([bs_pos, bs_neg]))
    return out

def auc_ci_boot(y, p, idx_list, alpha=0.95):
    """Bootstrap AUROC mean and percentile CI."""
    y = np.asarray(y); p = np.asarray(p)
    aucs = []
    for idx in idx_list:
        yb = y[idx]; pb = p[idx]
        if np.unique(yb).size < 2:
            continue
        aucs.append(roc_auc_score(yb, pb))
    aucs = np.asarray(aucs, dtype=float)
    lo, hi = np.percentile(aucs, [(1-alpha)/2*100, (1+alpha)/2*100])
    return float(aucs.mean()), float(lo), float(hi)

def metrics_at_threshold(y, p, thr):
    """Sensitivity, specificity, PPV, NPV and confusion matrix entries at a threshold."""
    y = np.asarray(y); p = np.asarray(p)
    pred = (p >= thr).astype(int)
    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()
    sens = tp / (tp + fn) if (tp + fn) else 0.0
    spec = tn / (tn + fp) if (tn + fp) else 0.0
    ppv  = tp / (tp + fp) if (tp + fp) else 0.0
    npv  = tn / (tn + fn) if (tn + fn) else 0.0
    return sens, spec, ppv, npv, tn, fp, fn, tp

def metrics_ci_boot(y, p, thr, idx_list):
    """Bootstrap CIs for threshold-based metrics."""
    sens_b, spec_b, ppv_b, npv_b = [], [], [], []
    for idx in idx_list:
        yb = np.asarray(y)[idx]; pb = np.asarray(p)[idx]
        if np.unique(yb).size < 2:
            continue
        s, c, ppv, npv, *_ = metrics_at_threshold(yb, pb, thr)
        sens_b.append(s); spec_b.append(c); ppv_b.append(ppv); npv_b.append(npv)
    def ci(a):
        a = np.asarray(a, dtype=float)
        lo, hi = np.percentile(a, [2.5, 97.5])
        return float(a.mean()), float(lo), float(hi)
    return {
        "sensitivity": ci(sens_b),
        "specificity": ci(spec_b),
        "PPV": ci(ppv_b),
        "NPV": ci(npv_b),
    }

def calibration_stats(y, p):
    """Logistic recalibration on logit(p): return intercept, slope, and Brier."""
    eps = 1e-6
    p_clip = np.clip(p, eps, 1 - eps)
    logit_p = np.log(p_clip / (1 - p_clip)).reshape(-1, 1)
    lr = LogisticRegression(
    solver="lbfgs", C=1e6, max_iter=1000,
    class_weight="balanced",
    random_state=SEED)
    lr.fit(logit_p, y)
    intercept = float(lr.intercept_[0])
    slope = float(lr.coef_.ravel()[0])
    brier = float(brier_score_loss(y, p_clip))
    return intercept, slope, brier

def logit(p):
    p = np.clip(p, 1e-6, 1 - 1e-6)
    return np.log(p / (1 - p))

# DeLong for correlated ROC curves
def _phi_matrix(pos_scores: np.ndarray, neg_scores: np.ndarray) -> np.ndarray:
    pos = pos_scores.reshape(-1, 1)
    neg = neg_scores.reshape(1, -1)
    gt = (pos > neg).astype(float)
    eq = (pos == neg).astype(float)
    return gt + 0.5 * eq

def _auc_v10_v01(y_true: np.ndarray, scores: np.ndarray):
    y_true = np.asarray(y_true).astype(int)
    scores = np.asarray(scores, dtype=float)
    pos_scores = scores[y_true == 1]
    neg_scores = scores[y_true == 0]
    m = len(pos_scores); n = len(neg_scores)
    if m < 2 or n < 2:
        raise ValueError("Need at least two positive and two negative cases.")
    phi = _phi_matrix(pos_scores, neg_scores)
    auc = phi.mean()
    V10 = phi.mean(axis=1)
    V01 = phi.mean(axis=0)
    return auc, V10, V01

def delong_test_correlated(y_true: np.ndarray, scores1: np.ndarray, scores2: np.ndarray):
    """DeLong test and CIs for two correlated ROC AUCs on the same sample."""
    y = np.asarray(y_true).astype(int)
    s1 = np.asarray(scores1, dtype=float)
    s2 = np.asarray(scores2, dtype=float)

    auc1, V10_1, V01_1 = _auc_v10_v01(y, s1)
    auc2, V10_2, V01_2 = _auc_v10_v01(y, s2)

    m = len(V10_1); n = len(V01_1)
    var1 = np.var(V10_1, ddof=1)/m + np.var(V01_1, ddof=1)/n
    var2 = np.var(V10_2, ddof=1)/m + np.var(V01_2, ddof=1)/n
    cov_V10 = np.cov(V10_1, V10_2, ddof=1)[0,1] / m
    cov_V01 = np.cov(V01_1, V01_2, ddof=1)[0,1] / n
    cov12   = cov_V10 + cov_V01

    delta = auc1 - auc2
    var_diff = max(var1 + var2 - 2.0 * cov12, 1e-12)
    z = delta / np.sqrt(var_diff)
    p = 2.0 * (1.0 - norm.cdf(abs(z)))

    auc1_ci = (auc1 - 1.96*np.sqrt(var1),  auc1 + 1.96*np.sqrt(var1))
    auc2_ci = (auc2 - 1.96*np.sqrt(var2),  auc2 + 1.96*np.sqrt(var2))
    delta_ci = (delta - 1.96*np.sqrt(var_diff), delta + 1.96*np.sqrt(var_diff))

    return {
        "auc1": auc1, "auc1_ci": auc1_ci,
        "auc2": auc2, "auc2_ci": auc2_ci,
        "delta": delta, "delta_ci": delta_ci,
        "z": z, "p": p, "var_diff": var_diff
    }

def pick_threshold_constrained(y, p, fnr_max=0.05, fpr_max=0.50, prefer="min_fpr"):
    """Pick OOF threshold with FNR ≤ fnr_max and FPR ≤ fpr_max."""
    y = np.asarray(y); p = np.asarray(p)
    fpr, tpr, thr = roc_curve(y, p)
    fnr = 1.0 - tpr
    ok = (fnr <= fnr_max) & (fpr <= fpr_max)
    if ok.any():
        idx_ok = np.where(ok)[0]
        if prefer == "youden":
            i = idx_ok[np.argmax(tpr[idx_ok] - fpr[idx_ok])]
        else:
            i = idx_ok[np.argmin(fpr[idx_ok])]
    else:
        viol = np.square(np.clip(fnr - fnr_max, 0, None)) + np.square(np.clip(fpr - fpr_max, 0, None))
        i = int(np.argmin(viol))
    return float(thr[i])


def _minutes_diff(t_end, t_start):
    dt_end = pd.to_datetime(t_end, errors="coerce")
    dt_start = pd.to_datetime(t_start, errors="coerce")
    return (dt_end - dt_start).dt.total_seconds() / 60.0

def _fit_l1_lr_with_cv(X, y, base_cv, Cs=None, l1_ratio=None, random_state=SEED):
    if Cs is None:
        Cs = np.logspace(-3, 2, 12)
    best_C, best_score = None, -np.inf
    for C in Cs:
        if l1_ratio is None:
            model = LogisticRegression(penalty="l1", solver="liblinear", C=C,
                                       max_iter=2000, random_state=random_state)
        else:
            model = LogisticRegression(penalty="elasticnet", solver="saga", C=C,
                                       l1_ratio=l1_ratio, max_iter=2000, random_state=random_state)
        scores = cross_val_score(model, X, y, cv=base_cv, scoring="roc_auc")
        m = float(np.mean(scores))
        if m > best_score:
            best_score, best_C = m, C
    if l1_ratio is None:
        return LogisticRegression(penalty="l1", solver="liblinear", C=best_C,
                                  max_iter=2000, random_state=random_state)
    else:
        return LogisticRegression(penalty="elasticnet", solver="saga", C=best_C,
                                  l1_ratio=l1_ratio, max_iter=2000, random_state=random_state)
def stability_select_features(X, y, R=100, subsample=0.5, threshold=0.6,
                              base_cv=None, l1_ratio=None, seed=SEED):
    if base_cv is None:
        base_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
    rng = np.random.default_rng(seed)
    counts = Counter()
    sss = StratifiedShuffleSplit(n_splits=R, train_size=subsample, random_state=seed)
    for r, (idx_tr, _) in enumerate(sss.split(X, y)):
        Xr, yr = X.iloc[idx_tr], y.iloc[idx_tr]
        model = _fit_l1_lr_with_cv(Xr, yr, base_cv=base_cv, l1_ratio=l1_ratio, random_state=seed+r)
        model.fit(Xr, yr)
        nz = np.where(np.abs(model.coef_.ravel()) > 1e-12)[0]
        for j in nz:
            counts[X.columns[j]] += 1
    freq = {f: counts[f] / R for f in X.columns}
    feats = [f for f, q in freq.items() if q >= threshold]
    # Sort by decreasing selection frequency
    feats.sort(key=lambda f: (-freq[f], f))
    return feats, freq


def preprocess(df_all: pd.DataFrame) -> pd.DataFrame:
    df = df_all.copy()

    if "time_difference" not in df.columns:
        if ("救急隊現着時間" in df.columns) and ("病着時間" in df.columns):
            df["time_difference"] = _minutes_diff(df["病着時間"], df["救急隊現着時間"])

    if "PS_interval" not in df.columns:
        if ("PS終了時間" in df.columns) and ("病着時間" in df.columns):
            df["PS_interval"] = _minutes_diff(df["PS終了時間"], df["病着時間"])

    for c in ["time_difference", "PS_interval"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
            df.loc[df[c] <= 0, c] = np.nan

    # --- Site-specific preprocessing (Nara EMR).
    # NOTE: If your dataset already uses English column names / standardized codes,
    # you may safely delete or comment out the relevant blocks below.
    # All operations are guarded by "if col in df.columns" / "errors='ignore'".

    # (A) Drop columns not used in this study (IDs, timestamps, PII, local admin fields).
    #     Adjust or remove to match your database. Safe to keep even if some names are absent.
    drop_cols = ["受傷日","退院日","予後","氏名","疾患カテゴリ","主病名","静脈疑い","受傷時間",
                 "交通事故_相手","交通事故_本人","病着時間","救急隊現着時間","PS終了時間"]
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

    # (B) Rename Japanese headers used at Nara EMR to short English tokens used downstream.
    #     If your columns are already in English, you can skip this entire rename map.
    df = df.rename(columns={
        '性別２':"Sex",'年齢':"Age","常用薬":"medicine",'搬入手段':"transport",
        "Blunt_Injury_除外":"mechanism_pre",
        '現着DBP':"scene_DBP",'現着HR':"scene_HR",'現着RR':"scene_RR",'現着SBP':"scene_SBP",
        '病着DBP':"hos_DBP",'病着HR':"hos_HR",'病着RR':"hos_RR",'病着SBP':"hos_SBP",
        "BEインポート":"BE","HCO3インポート":"HCO3",
        'PS後DBP':"postPS_DBP",'PS後HR':"postPS_HR",'PS後RR':"postPS_RR",'PS後SBP':"postPS_SBP",
        "低体温":"hypothermia","切迫するD":"neuro_disability",
        '病院前LSI':"prehosLSI","来院後LSI":"hosLSI",
        "hos緊急止血術":"S+IVR"
    })

    # (C) Mechanism extraction from a free‑text or multi‑valued field to a coarse category.
    #     If you already have a clean categorical "mechanism", skip this block.
    def extract_relevant_category(text):
        if pd.isna(text):
            return None
        for cat in ["交通事故","転倒","転落墜落","鋭的","その他"]:
            if cat in str(text):
                return cat
        return None
    if "mechanism_pre" in df.columns:
        df["mechanism"] = df["mechanism_pre"].apply(extract_relevant_category)
        df = df.drop(columns=["mechanism_pre"], errors="ignore")

    # (D) Value normalization: map local strings to standardized codes (0/1 or canonical labels).
    #     If your values are already normalized (e.g., Sex in {0,1}, FAST in {"Neg","Pos","Not"}),
    #     you can remove relevant entries from this replace() or skip entirely.
    df = df.replace({
        "Sex":{"M":1, "Ｍ":1, "F":0, "Ｆ":0},
        "prehosFAST":{"陰性":"Neg","陽性":"Pos","未実施":"Not"},
        "hosFAST":{"陰性":"Neg","陽性":"Pos","未実施":"Not"},
        "neuro_disability":{"No":0,"Yes":1},
        "hypothermia":{"Yes":1,"No":0},
        "mechanism":{
            "交通事故":"Traffic_Accident","鋭的":"Penetrating_Trauma",
            "転倒":"Fall","転落墜落":"Fall_from_height","その他":"Other"
        },
        # Transport codes are site-specific; adjust or drop as needed.
        "transport":{"救急車":"EMS","ドクターカー":"Drcar","ドクターヘリ":"HEM","その他":"other"}
    })

    # (E) Outcome mapping (CRITICAL): map local procedure strings to binary label {0,1}.
    #     Adjust to your center’s coding. If "S+IVR" is already 0/1, skip this block.
    s_map = {"なし":0, "CT前Surg":0, "ECMO":0, "頭部Surg":0, "非止血整形手術":0,
             "頭部Surg\nなし":0, "なし\n頭部Surg":0, 'なし\n非止血整形手術':0,
             "なし\n非止血整形手術\n頭部Surg":0, "非止血整形手術\nなし":0,
             "体幹Surgery":1,"体幹IVR":1,"体幹Surg→IVR":1,"骨盤四肢Surg":1,
             "骨盤四肢Surg\n体幹IVR→Surg":1,"骨盤四肢Surg\n体幹Surgery":1,
             "骨盤四肢Surg\n体幹IVR":1,'体幹IVR\n骨盤四肢Surg':1,"体幹IVR→Surg":1,
             "頭部Surg\n体幹IVR":1,"体幹IVR\n非止血整形手術":1,"体幹Surgery\n骨盤四肢Surg":1,
             "非止血整形手術\n体幹IVR":1,"体幹IVR\n頭部Surg":1}
    if "S+IVR" in df.columns:
        df["S+IVR"] = df["S+IVR"].map(s_map).astype("Int64")

    # (F) Multi‑select fields → one‑hot. Local DB uses newline‑separated entries.
    #     If your DB uses a different delimiter (e.g., comma), change "split('\n')" accordingly.
    #     If you don’t have these columns, this block is a no‑op.
    repl = {
        "prehosLSI":{"なし":"none","挿管":"intubate","胸腔ドレナージ":"chest_tube",
                     "タニケット":"tanicket","骨盤バインダー":"pelvic_binder","IABO":"IABO"},
        "hosLSI":{"なし":"none","挿管":"intubate","胸腔ドレナージ":"chest_tube",
                  "タニケット":"tanicket","骨盤バインダー":"pelvic_binder","IABO":"IABO"},
        "medicine":{"DM薬":"DM","抗血栓薬":"anticoag","該当なし":"none","βブロッカ":"beta"}
    }
    for col in ["prehosLSI","hosLSI","medicine"]:
        if col in df.columns:
            df[col] = df[col].fillna("").apply(
                lambda x: [repl[col].get(item, item) for item in str(x).split("\n")]
            )
            df[col] = df[col].apply(set)
            mlb = MultiLabelBinarizer()
            enc = pd.DataFrame(mlb.fit_transform(df[col]),
                               columns=[f"{col}_{c}" for c in mlb.classes_],
                               index=df.index)
            df = pd.concat([df, enc], axis=1)
    df = df.drop(columns=[c for c in ["prehosLSI","hosLSI","medicine"] if c in df.columns], errors="ignore")

    # (G) FAST as two variables: performed? and positive?
    #     Keep if you want to separate availability from positivity. Otherwise you can retain a single field.
    if "prehosFAST" in df.columns:
        df["prehosFAST_performed"] = (df["prehosFAST"] != "Not").astype(int)
        df["prehosFAST_pos"]       = (df["prehosFAST"] == "Pos").astype(int)
    if "hosFAST" in df.columns:
        df["hosFAST_performed"] = (df["hosFAST"] != "Not").astype(int)
        df["hosFAST_pos"]       = (df["hosFAST"] == "Pos").astype(int)

    # (H) One‑hot encode categoricals. Adjust the list to your actual categorical columns.
    for cat_col in ["transport","mechanism"]:
        if cat_col in df.columns:
            df = pd.concat([df, pd.get_dummies(df[cat_col], prefix=cat_col, dtype=int)], axis=1)
    df = df.drop(columns=[c for c in ["transport","prehosFAST","hosFAST","mechanism"] if c in df.columns],
                 errors="ignore")

    # (I) Derived features: shock index and time‑rate deltas.
    #     REQUIRED columns: "time_difference" (scene→arrival minutes) and "PS_interval" (arrival→end‑PS minutes).
    #     If you cannot compute these durations in your data, either add them upstream or remove the delta features
    #     and drop them from the modeling/scaling lists accordingly.
    needed = {"time_difference","PS_interval"}
    if not needed.issubset(df.columns):
        missing = list(needed - set(df.columns))
        raise ValueError(f"Required time columns are missing: {missing}")

    for a,b,out in [("scene_HR","scene_SBP","scene_SI"),
                    ("hos_HR","hos_SBP","hos_SI"),
                    ("postPS_HR","postPS_SBP","postPS_SI")]:
        if a in df.columns and b in df.columns:
            df[out] = df[a] / df[b]
    # Delta per minute; protect against zero/short intervals.
    eps1 = df["time_difference"].clip(lower=1.0)
    eps2 = df["PS_interval"].clip(lower=1.0)

    df["HR_delta_1"]  = (df["hos_HR"]    - df["scene_HR"])  / eps1
    df["SBP_delta_1"] = (df["hos_SBP"]   - df["scene_SBP"]) / eps1
    df["DBP_delta_1"] = (df["hos_DBP"]   - df["scene_DBP"]) / eps1
    df["RR_delta_1"]  = (df["hos_RR"]    - df["scene_RR"])  / eps1
    df["SI_delta_1"]  = (df["hos_SI"]    - df["scene_SI"])  / eps1

    df["HR_delta_2"]  = (df["postPS_HR"]   - df["hos_HR"])  / eps2
    df["SBP_delta_2"] = (df["postPS_SBP"]  - df["hos_SBP"]) / eps2
    df["DBP_delta_2"] = (df["postPS_DBP"]  - df["hos_DBP"]) / eps2
    df["RR_delta_2"]  = (df["postPS_RR"]   - df["hos_RR"])  / eps2
    df["SI_delta_2"]  = (df["postPS_SI"]   - df["hos_SI"])  / eps2

    # (J) Winsorization to dampen extreme delta values (1st–99th pct).
    #     Optional; you can relax/tighten the percentiles or remove if not desired.
    for c in ["HR_delta_1","SBP_delta_1","DBP_delta_1","RR_delta_1","SI_delta_1",
              "HR_delta_2","SBP_delta_2","DBP_delta_2","RR_delta_2","SI_delta_2"]:
        if c in df.columns:
            q1, q99 = df[c].quantile([0.01, 0.99])
            df[c] = df[c].clip(lower=q1, upper=q99)
    # (K) Cleanup of local-only columns (Nara). Safe to ignore if absent.
    df = df.drop(columns=[c for c in ["time_difference","PS_interval","PS後SI"] if c in df.columns],
                 errors="ignore")

    # (L) Finalize label and dtypes. If label is already numeric, this is a no‑op.
    df = df.dropna(subset=["S+IVR"])
    df["S+IVR"] = df["S+IVR"].astype(int)
    if "Sex" in df.columns:
        df["Sex"] = df["Sex"].astype(int)
    return df

def standardize_train_test(x_train, x_test):
    """Standardize continuous fields used by LR baseline and for stability."""
    scaling_columns = [
        'Age','BE','HCO3','Lac','pH','Hb',
        'postPS_DBP','postPS_HR','postPS_RR','postPS_SBP',
        'scene_DBP','scene_HR','scene_RR','scene_SBP',
        'hos_DBP','hos_HR','hos_RR','hos_SBP',
        "HR_delta_1","SBP_delta_1","DBP_delta_1","RR_delta_1","SI_delta_1",
        "HR_delta_2","SBP_delta_2","DBP_delta_2","RR_delta_2","SI_delta_2",
        "scene_SI","hos_SI","postPS_SI"
    ]
    cols = [c for c in scaling_columns if c in x_train.columns]
    sc = StandardScaler().fit(x_train[cols])
    x_train[cols] = sc.transform(x_train[cols])
    x_test[cols]  = sc.transform(x_test[cols])
    return x_train, x_test


# ===== LightGBM tuning, training, and calibration =====
def tune_lgbm(x_train, y_train, cv_splits):
    fixed_params = {
        "boosting_type":"gbdt","objective":"binary","metric":"auc",
        "verbosity":-1,"n_estimators":5000,"seed":SEED,
        "bagging_seed":SEED,"feature_fraction_seed":SEED,"n_jobs":1
    }
    def objective(trial):
        pos = int(y_train.sum()); neg = len(y_train) - pos
        base_spw = neg / max(pos, 1)
        low  = max(1.0, base_spw * 0.33)
        high = base_spw * 3.0
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.02, 0.06),
            "num_leaves": trial.suggest_int("num_leaves", 31, 255),
            "max_depth": trial.suggest_int("max_depth", -1, 10),
            "min_child_samples": trial.suggest_int("min_child_samples", 5, 60),
            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 5, 60),
            "min_sum_hessian_in_leaf": trial.suggest_float("min_sum_hessian_in_leaf", 1e-6, 1e-3, log=True),
            "feature_fraction": trial.suggest_float("feature_fraction", 0.80, 1.00),
            "bagging_fraction": trial.suggest_float("bagging_fraction", 0.80, 1.00),
            "bagging_freq": trial.suggest_int("bagging_freq", 1, 3),
            "lambda_l1": trial.suggest_float("lambda_l1", 1e-3, 10, log=True),
            "lambda_l2": trial.suggest_float("lambda_l2", 1e-3, 100, log=True),
            "scale_pos_weight": trial.suggest_float("scale_pos_weight", low, high, log=True),
            "extra_trees": trial.suggest_categorical("extra_trees", [False, True]),
            "path_smooth": trial.suggest_float("path_smooth", 0.0, 20.0),
        }
        params = {**fixed_params, **params}
        aucs = []
        for tr_idx, va_idx in cv_splits:
            x_tr, y_tr = x_train.iloc[tr_idx], y_train.iloc[tr_idx]
            x_va, y_va = x_train.iloc[va_idx], y_train.iloc[va_idx]
            model = lgb.LGBMClassifier(**params)
            model.fit(x_tr, y_tr, eval_set=[(x_va, y_va)],
                      callbacks=[lgb.early_stopping(stopping_rounds=ES_ROUNDS, verbose=-1)])
            y_va_pred = model.predict_proba(x_va)[:, 1]
            aucs.append(roc_auc_score(y_va, y_va_pred))
        return float(np.mean(aucs))
    sampler = optuna.samplers.TPESampler(seed=SEED)
    study = optuna.create_study(direction="maximize", sampler=sampler)
    study.optimize(objective, n_trials=100, show_progress_bar=False)
    best_params = study.best_params
    return {**fixed_params, **best_params}

def train_lgbm_and_predict(x_train, y_train, x_test, params, cv_splits):
    """Return OOF probabilities and mean test probabilities across folds."""
    oof_pred = np.zeros(y_train.shape[0], dtype=float)
    test_pred_folds = np.zeros((x_test.shape[0], len(cv_splits)), dtype=float)
    val_auc_scores = []
    for k, (tr_idx, va_idx) in enumerate(cv_splits):
        x_tr, y_tr = x_train.iloc[tr_idx], y_train.iloc[tr_idx]
        x_va, y_va = x_train.iloc[va_idx], y_train.iloc[va_idx]
        model = lgb.LGBMClassifier(**params)
        model.fit(x_tr, y_tr, eval_set=[(x_va, y_va)],
                  callbacks=[lgb.early_stopping(stopping_rounds=ES_ROUNDS, verbose=-1)])
        oof_pred[va_idx] = model.predict_proba(x_va)[:, 1]
        test_pred_folds[:, k] = model.predict_proba(x_test)[:, 1]
        val_auc_scores.append(roc_auc_score(y_va, oof_pred[va_idx]))
    test_pred_mean = test_pred_folds.mean(axis=1)
    return oof_pred, test_pred_mean, val_auc_scores

def platt_recalibration(p_train, y_train, p_test):
    """Platt-type logistic recalibration on logit probabilities."""
    lr = LogisticRegression(
    solver="lbfgs", C=1e6, max_iter=1000,
    class_weight="balanced",
    random_state=SEED)
    lr.fit(logit(p_train).reshape(-1,1), y_train)
    p_train_cal = lr.predict_proba(logit(p_train).reshape(-1,1))[:,1]
    p_test_cal  = lr.predict_proba(logit(p_test).reshape(-1,1))[:,1]
    return p_train_cal, p_test_cal

# ===== Logistic regression baseline =====
def tune_fit_logreg(x_train, y_train, cv_splits):
    def make_lr(params):
        C = params["C"]; penalty = params["penalty"]
        if penalty == "elasticnet":
            return LogisticRegression(C=C, penalty="elasticnet", l1_ratio=params["l1_ratio"],
                                      solver="saga", max_iter=1000, random_state=SEED)
        elif penalty == "l1":
            return LogisticRegression(C=C, penalty="l1", solver="liblinear",
                                      max_iter=1000, random_state=SEED)
        elif penalty == "l2":
            return LogisticRegression(C=C, penalty="l2", solver="lbfgs",
                                      max_iter=1000, random_state=SEED)
        else:
            return LogisticRegression(penalty=None, solver="lbfgs",
                                      max_iter=1000, random_state=SEED)

    def objective(trial):
        C = trial.suggest_float("C", 1e-6, 1e3, log=True)
        penalty = trial.suggest_categorical("penalty", ["l1","l2","elasticnet", None])
        params = {"C":C, "penalty":penalty}
        if penalty == "elasticnet":
            params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)
        model = make_lr(params)
        scores = cross_val_score(model, x_train, y_train, cv=list(cv_splits), scoring="roc_auc")
        return float(np.mean(scores))

    study = optuna.create_study(direction="maximize", sampler=optuna.samplers.TPESampler(seed=SEED))
    study.optimize(objective, n_trials=100, show_progress_bar=False)
    best = study.best_params
    if "l1_ratio" not in best:
        best["l1_ratio"] = None
    model = make_lr(best)
    model.fit(x_train, y_train)
    return model

# ===== Plots =====
def show_roc_plot(y_test, p_lgbm, p_lr):
    fpr_lgb, tpr_lgb, _ = roc_curve(y_test, p_lgbm)
    fpr_lr,  tpr_lr,  _ = roc_curve(y_test, p_lr)
    roc_auc_lgb = auc(fpr_lgb, tpr_lgb)
    roc_auc_lr  = auc(fpr_lr, tpr_lr)
    tpr_at05_lgb = np.interp(0.5, fpr_lgb, tpr_lgb)
    tpr_at05_lr  = np.interp(0.5, fpr_lr,  tpr_lr)
    fig, ax = plt.subplots(figsize=(6,6))
    ax.plot(fpr_lgb, tpr_lgb, lw=2, label=f'LightGBM AUC {roc_auc_lgb:.2f}')
    ax.plot(fpr_lr,  tpr_lr,  lw=2, label=f'Logistic AUC {roc_auc_lr:.2f}')
    ax.plot([0,1],[0,1], linestyle="--", color="gray", lw=1)
    ax.plot([0.5,0.5],[0,tpr_at05_lgb], color='C0', linestyle=':', lw=1)
    ax.plot([0,0.5],[tpr_at05_lgb,tpr_at05_lgb], color='C0', linestyle=':', lw=1)
    ax.plot([0.5,0.5],[0,tpr_at05_lr],  color='C1', linestyle=':', lw=1)
    ax.plot([0,0.5],[tpr_at05_lr, tpr_at05_lr],  color='C1', linestyle=':', lw=1)
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title("ROC curves on the independent test set")
    ax.legend(loc="lower right")
    plt.show()

def show_calibration_plot(y_test, p_pre, p_post):
    prob_true_raw, prob_pred_raw = calibration_curve(y_test, p_pre, n_bins=10, strategy="quantile")
    prob_true_cal, prob_pred_cal = calibration_curve(y_test, p_post, n_bins=10, strategy="quantile")
    cal_int_pre,  cal_slope_pre,  brier_pre  = calibration_stats(y_test, p_pre)
    cal_int_post, cal_slope_post, brier_post = calibration_stats(y_test, p_post)
    fig, ax = plt.subplots(figsize=(5,5))
    ax.plot([0,1],[0,1], linestyle="--", lw=1.2, label="Perfect calibration")
    ax.plot(prob_pred_raw, prob_true_raw, marker="o", lw=1.5, label="Pre calibration")
    ax.plot(prob_pred_cal, prob_true_cal, marker="s", lw=1.5, label="Post calibration")
    ax.set_xlabel("Predicted probability")
    ax.set_ylabel("Observed frequency")
    ax.set_title("Calibration on the independent test set")
    ax.legend(loc="lower right")
    text_str = (
        f"Pre  Intercept {cal_int_pre:.2f}, Slope {cal_slope_pre:.2f}, Brier {brier_pre:.3f}\n"
        f"Post Intercept {cal_int_post:.2f}, Slope {cal_slope_post:.2f}, Brier {brier_post:.3f}"
    )
    ax.text(0.05, 0.78, text_str, transform=ax.transAxes)
    plt.show()

def append_row(rows, dataset, model, thr, auc_mean, auc_lo, auc_hi, m_dict, cal_tuple):
    rows.append({
        "Dataset": dataset,
        "Model": model,
        "Threshold": thr,
        "AUROC": round(auc_mean, 3),
        "AUROC_CI_low": round(auc_lo, 3),
        "AUROC_CI_high": round(auc_hi, 3),
        "Sensitivity_mean": round(m_dict["sensitivity"][0], 3),
        "Sensitivity_CI_low": round(m_dict["sensitivity"][1], 3),
        "Sensitivity_CI_high": round(m_dict["sensitivity"][2], 3),
        "Specificity_mean": round(m_dict["specificity"][0], 3),
        "Specificity_CI_low": round(m_dict["specificity"][1], 3),
        "Specificity_CI_high": round(m_dict["specificity"][2], 3),
        "PPV_mean": round(m_dict["PPV"][0], 3),
        "PPV_CI_low": round(m_dict["PPV"][1], 3),
        "PPV_CI_high": round(m_dict["PPV"][2], 3),
        "NPV_mean": round(m_dict["NPV"][0], 3),
        "NPV_CI_low": round(m_dict["NPV"][1], 3),
        "NPV_CI_high": round(m_dict["NPV"][2], 3),
        "Calib_intercept": round(cal_tuple[0], 3),
        "Calib_slope": round(cal_tuple[1], 3),
        "Brier": round(cal_tuple[2], 3),
    })

# ===== Run: main analysis =====
if "df_all" not in globals():
    raise RuntimeError("Please load your dataset into a pandas DataFrame named `df_all` before running.")

# Preprocess
df = preprocess(df_all)

# Label and features
y = df["S+IVR"].astype(int)
X = df.drop(columns=["S+IVR"])
if "ID" in X.columns:
    X = X.drop(columns=["ID"])

# 80:20 stratified split
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=SEED)

# Standardize continuous variables
X_tr, X_te = standardize_train_test(X_tr.copy(), X_te.copy())

# Aliases (ensure later blocks use the standardized matrices)
x_train, y_train = X_tr, y_tr
x_test,  y_test  = X_te, y_te

# CV folds
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
cv_splits = list(cv.split(X_tr, y_tr))

cv_splits_full = cv_splits

# LightGBM tuning and training
lgbm_params = tune_lgbm(X_tr, y_tr, cv_splits)
oof_lgbm_raw, te_lgbm_raw, _ = train_lgbm_and_predict(X_tr, y_tr, X_te, lgbm_params, cv_splits)

# Platt-type recalibration on logit probabilities
oof_lgbm_cal, te_lgbm_cal = platt_recalibration(oof_lgbm_raw, y_tr, te_lgbm_raw)

# Logistic regression baseline
lr_model = tune_fit_logreg(X_tr, y_tr, cv_splits)
te_lr = lr_model.predict_proba(X_te)[:, 1]

# AUROC with bootstrap CI
idx_cv = stratified_bootstrap_indices(y_tr, n_boot=5000, seed=SEED)
idx_te = stratified_bootstrap_indices(y_te, n_boot=5000, seed=SEED)
auc_cv_lgbm, lo_cv_lgbm, hi_cv_lgbm = auc_ci_boot(y_tr, oof_lgbm_cal, idx_cv)
auc_te_lgbm, lo_te_lgbm, hi_te_lgbm = auc_ci_boot(y_te, te_lgbm_cal, idx_te)
auc_te_lr,   lo_te_lr,   hi_te_lr   = auc_ci_boot(y_te, te_lr,       idx_te)

print(f"[LightGBM] Test AUROC {auc_te_lgbm:.3f} [{lo_te_lgbm:.3f}, {hi_te_lgbm:.3f}]")
print(f"[Logistic ] Test AUROC {auc_te_lr:.3f} [{lo_te_lr:.3f}, {hi_te_lr:.3f}]")

# DeLong test on the same test set
res = delong_test_correlated(y_te, te_lgbm_cal, te_lr)
delta = res["delta"]
se    = np.sqrt(res["var_diff"])
ci_lo = delta - 1.96*se
ci_hi = delta + 1.96*se
print(f"[DeLong] ΔAUC {delta:.3f} [{ci_lo:.3f}, {ci_hi:.3f}], p={res['p']:.3f}")
print(f"[DeLong] LGBM AUC {res['auc1']:.3f} [{res['auc1_ci'][0]:.3f}, {res['auc1_ci'][1]:.3f}]")
print(f"[DeLong]  LR  AUC {res['auc2']:.3f} [{res['auc2_ci'][0]:.3f}, {res['auc2_ci'][1]:.3f}]")

# Choose operating threshold
thr_use = FIXED_THRESHOLD if USE_FIXED_THRESHOLD else \
          pick_threshold_constrained(y_tr, oof_lgbm_cal, fnr_max=0.05, fpr_max=0.50, prefer="min_fpr")
print(f"[Threshold] Operating threshold used: {thr_use:.3f}")

# Metrics and calibration at the chosen threshold
m_cv_lgbm = metrics_ci_boot(y_tr, oof_lgbm_cal, thr_use, idx_cv)
m_te_lgbm = metrics_ci_boot(y_te, te_lgbm_cal, thr_use, idx_te)
m_te_lr   = metrics_ci_boot(y_te, te_lr,        thr_use, idx_te)

cal_cv_lgbm = calibration_stats(y_tr, oof_lgbm_cal)
cal_te_lgbm = calibration_stats(y_te, te_lgbm_cal)
cal_te_lr   = calibration_stats(y_te, te_lr)

# Performance summary table (display only)
rows = []
append_row(rows, "Train or validation (OOF)", "LightGBM", thr_use,
           auc_cv_lgbm, lo_cv_lgbm, hi_cv_lgbm, m_cv_lgbm, cal_cv_lgbm)
append_row(rows, "Independent test", "LightGBM", thr_use,
           auc_te_lgbm, lo_te_lgbm, hi_te_lgbm, m_te_lgbm, cal_te_lgbm)
append_row(rows, "Independent test", "Logistic regression", thr_use,
           auc_te_lr, lo_te_lr, hi_te_lr, m_te_lr, cal_te_lr)
perf_df = pd.DataFrame(rows)
display(perf_df)

# Calibration plot and ROC curves
show_calibration_plot(y_te, te_lgbm_raw, te_lgbm_cal)
show_roc_plot(y_te, te_lgbm_cal, te_lr)

# Confusion matrix at the operating threshold
sens, spec, ppv, npv, tn, fp, fn, tp = metrics_at_threshold(y_te, te_lgbm_cal, thr_use)
print("\nConfusion matrix at threshold", thr_use)
print(f"TP {tp}  FN {fn}  FP {fp}  TN {tn}")
print(f"Sensitivity {sens:.3f}  Specificity {spec:.3f}  PPV {ppv:.3f}  NPV {npv:.3f}")

# ===== Decision curve analysis (DCA) =====
def net_benefit(y_true, p, thresholds, per100=False):
    """Net benefit at each threshold (per patient by default)."""
    y = np.asarray(y_true, dtype=int)
    p = np.asarray(p, dtype=float)
    N = len(y)
    nb = []
    for pt in thresholds:
        pred = (p >= pt).astype(int)
        tp = np.sum((pred == 1) & (y == 1))
        fp = np.sum((pred == 1) & (y == 0))
        w = pt / (1.0 - pt)
        val = (tp / N) - (fp / N) * w
        nb.append(val * (100 if per100 else 1))
    return np.array(nb)

def dca_curves(y_true, p, thresholds, per100=False):
    """Return model, Treat-all, and Treat-none net benefit curves."""
    prev = float(np.mean(y_true))
    model = net_benefit(y_true, p, thresholds, per100=per100)
    treat_all = (prev - (1 - prev) * (thresholds / (1 - thresholds))) * (100 if per100 else 1)
    treat_none = np.zeros_like(thresholds)
    return model, treat_all, treat_none

def dca_plot(y_true, p, thr_mark=None, title="Decision curve (independent test set)",
             per100=False, bootstrap_ci=True, n_boot=2000, seed=SEED):
    """Plot DCA with optional 95% bootstrap CI."""
    rng = np.random.default_rng(seed)
    thresholds = np.arange(0.001, 0.501, 0.001)
    model, allc, none = dca_curves(y_true, p, thresholds, per100=per100)

    plt.figure(figsize=(7.0, 5.5))
    plt.plot(thresholds, model, lw=1.8, label="Model")
    plt.plot(thresholds, allc,  '--', lw=1.2, color='gray', label="Treat all")
    plt.plot(thresholds, none,  ':',  lw=1.2, color='gray', label="Treat none")

    if bootstrap_ci:
        y_arr = np.asarray(y_true, dtype=int)
        p_arr = np.asarray(p, dtype=float)
        pos = np.where(y_arr == 1)[0]
        neg = np.where(y_arr == 0)[0]
        band = []
        for _ in range(n_boot):
            bs_pos = rng.choice(pos, size=len(pos), replace=True)
            bs_neg = rng.choice(neg, size=len(neg), replace=True)
            idx = np.concatenate([bs_pos, bs_neg])
            band.append(net_benefit(y_arr[idx], p_arr[idx], thresholds, per100=per100))
        band = np.stack(band, axis=0)
        lo, hi = np.percentile(band, [2.5, 97.5], axis=0)
        plt.fill_between(thresholds, lo, hi, alpha=0.18, label="Model 95% CI")

    if thr_mark is not None:
        plt.axvline(thr_mark, color='red', ls='--', lw=1.6,
                    label=f"Operating threshold ({thr_mark:.3f})")

    plt.axhline(0.0, color='k', ls='--', lw=0.8, alpha=0.5)
    plt.xlim(0.00, 0.50)
    plt.xticks(np.arange(0.00, 0.51, 0.05))
    plt.xlabel("Threshold probability (pt)")
    plt.ylabel("Net benefit" + (" (per 100 patients)" if per100 else " (per patient)"))
    plt.title(title)
    plt.grid(True, alpha=0.25)
    plt.legend(loc='best')
    plt.tight_layout()
    plt.show()

# Run DCA on LightGBM test predictions
dca_plot(y_true=y_te, p=te_lgbm_cal, thr_mark=thr_use,
         title="Decision curve analysis on the independent test set",
         per100=False, bootstrap_ci=True, n_boot=2000, seed=SEED)


def paired_bootstrap_auc_diff(y, s1, s2, idx_list):
    y = np.asarray(y); s1 = np.asarray(s1); s2 = np.asarray(s2)
    diffs = []
    for idx in idx_list:
        yb = y[idx]; s1b = s1[idx]; s2b = s2[idx]
        if np.unique(yb).size < 2:
            continue
        diffs.append(roc_auc_score(yb, s1b) - roc_auc_score(yb, s2b))
    diffs = np.asarray(diffs)
    lo, hi = np.percentile(diffs, [2.5, 97.5])
    p_two = 2 * min((diffs <= 0).mean(), (diffs >= 0).mean())
    return float(diffs.mean()), (float(lo), float(hi)), float(min(max(p_two,0.0),1.0))


# ===== SHAP beeswarm for 5 folds (no saving, display only) =====
import shap
def plot_shap_beeswarm_per_fold(X_tr, y_tr, cv_splits, lgbm_params, max_display=20):
    for fold_count, (tr_idx, va_idx) in enumerate(cv_splits, start=1):
        X_tr_fold, y_tr_fold = X_tr.iloc[tr_idx], y_tr.iloc[tr_idx]
        X_va_fold, y_va_fold = X_tr.iloc[va_idx], y_tr.iloc[va_idx]
        model = lgb.LGBMClassifier(**lgbm_params)
        model.fit(X_tr_fold, y_tr_fold,
                  eval_set=[(X_va_fold, y_va_fold)],
                  callbacks=[lgb.early_stopping(stopping_rounds=ES_ROUNDS, verbose=-1)])
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_va_fold)
        shap_to_plot = shap_values[1] if isinstance(shap_values, list) and len(shap_values) >= 2 else shap_values
        plt.figure(figsize=(7, 9))
        shap.summary_plot(shap_to_plot, X_va_fold, plot_type="dot", show=False, max_display=max_display)
        ax = plt.gca()
        ax.set_title(f"SHAP beeswarm (validation of fold {fold_count})", pad=10)
        plt.tight_layout()
        plt.show()

# show 5 figures
plot_shap_beeswarm_per_fold(X_tr, y_tr, cv_splits, lgbm_params, max_display=20)


# ===== Temporal hold-out analyses (run in a separate cell) =====

def add_index_year(df_raw: pd.DataFrame, date_col_override: str = None):
    """
    Add index_year from a date/time column in raw df before preprocess drops timestamp columns.
    Returns: (df_with_year, used_column or None)
    """
    df2 = df_raw.copy()

    candidates = [
        date_col_override,
        "受傷日",
        "受傷時間",
        "病着時間",
        "救急隊現着時間",
        "PS終了時間",
        "退院日",
    ]
    candidates = [c for c in candidates if (c is not None and c in df2.columns)]

    if len(candidates) == 0:
        df2["index_year"] = pd.NA
        return df2, None

    used = candidates[0]
    dt = pd.to_datetime(df2[used], errors="coerce")
    df2["index_year"] = dt.dt.year.astype("Int64")
    return df2, used


def make_cv_splits_safe(X: pd.DataFrame, y: pd.Series, max_splits: int = 5, seed: int = SEED):
    """Choose a StratifiedKFold with n_splits <= min class counts."""
    y_arr = np.asarray(y).astype(int)
    n_pos = int((y_arr == 1).sum())
    n_neg = int((y_arr == 0).sum())
    n_splits = min(max_splits, n_pos, n_neg)
    if n_splits < 2:
        return None
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
    return list(cv.split(X, y))


def run_temporal_holdout_lgbm(
    df_raw: pd.DataFrame,
    train_years,
    test_years,
    scenario_name: str,
    params_base: dict,
    thr: float,
    date_col_override: str = None,
):
    # 1) add index_year
    df_year, used_col = add_index_year(df_raw, date_col_override=date_col_override)
    if used_col is None:
        print(f"[Temporal] {scenario_name}: index_year を作れませんでした（日時列が見つからない）")
        return None

    # 2) preprocess
    dfp = preprocess(df_year)

    if "index_year" not in dfp.columns:
        print(f"[Temporal] {scenario_name}: preprocess 後に index_year が存在しません")
        return None

    dfp = dfp.dropna(subset=["index_year"]).copy()
    dfp["index_year"] = dfp["index_year"].astype(int)

    df_tr = dfp[dfp["index_year"].isin(train_years)].copy()
    df_te = dfp[dfp["index_year"].isin(test_years)].copy()

    n_tr = len(df_tr)
    n_te = len(df_te)
    e_tr = int(df_tr["S+IVR"].sum()) if n_tr else 0
    e_te = int(df_te["S+IVR"].sum()) if n_te else 0

    print(f"\n[Temporal] {scenario_name}")
    print(f"  date col for year: {used_col}")
    print(f"  Train years {train_years}: n={n_tr}, events={e_tr} ({(e_tr/n_tr if n_tr else 0):.3f})")
    print(f"  Test  years {test_years}:  n={n_te}, events={e_te} ({(e_te/n_te if n_te else 0):.3f})")

    if n_tr == 0 or n_te == 0:
        print("  -> Skip（分割が空）")
        return None

    y_tr = df_tr["S+IVR"].astype(int)
    X_tr = df_tr.drop(columns=["S+IVR"], errors="ignore")
    y_te = df_te["S+IVR"].astype(int)
    X_te = df_te.drop(columns=["S+IVR"], errors="ignore")

    X_tr = X_tr.drop(columns=[c for c in ["ID", "index_year"] if c in X_tr.columns], errors="ignore")
    X_te = X_te.drop(columns=[c for c in ["ID", "index_year"] if c in X_te.columns], errors="ignore")

    if np.unique(y_tr).size < 2 or np.unique(y_te).size < 2:
        print("  -> Skip（train/test ともに AUROC 計算に必要な2クラスが揃わない）")
        return None

    X_te = X_te.reindex(columns=X_tr.columns)

    X_tr, X_te = standardize_train_test(X_tr.copy(), X_te.copy())

    cv_splits = make_cv_splits_safe(X_tr, y_tr, max_splits=5, seed=SEED)
    if cv_splits is None:
        print("  -> Skip（CV splits を作れない: クラス数が少ない）")
        return None

    oof_raw, te_raw, _ = train_lgbm_and_predict(X_tr, y_tr, X_te, params_base, cv_splits)
    oof_cal, te_cal = platt_recalibration(oof_raw, y_tr, te_raw)

    idx_te = stratified_bootstrap_indices(y_te, n_boot=5000, seed=SEED)
    auc_mean, auc_lo, auc_hi = auc_ci_boot(y_te, te_cal, idx_te)

    m_te = metrics_ci_boot(y_te, te_cal, thr, idx_te)
    cal_te = calibration_stats(y_te, te_cal)
    sens, spec, ppv, npv, tn, fp, fn, tp = metrics_at_threshold(y_te, te_cal, thr)

    out = {
        "Scenario": scenario_name,
        "Train_years": str(train_years),
        "Test_years": str(test_years),
        "n_train": n_tr,
        "events_train": e_tr,
        "n_test": n_te,
        "events_test": e_te,
        "Threshold": float(thr),
        "AUROC": round(auc_mean, 3),
        "AUROC_CI_low": round(auc_lo, 3),
        "AUROC_CI_high": round(auc_hi, 3),
        "Sensitivity_mean": round(m_te["sensitivity"][0], 3),
        "Sensitivity_CI_low": round(m_te["sensitivity"][1], 3),
        "Sensitivity_CI_high": round(m_te["sensitivity"][2], 3),
        "Specificity_mean": round(m_te["specificity"][0], 3),
        "Specificity_CI_low": round(m_te["specificity"][1], 3),
        "Specificity_CI_high": round(m_te["specificity"][2], 3),
        "PPV_mean": round(m_te["PPV"][0], 3),
        "PPV_CI_low": round(m_te["PPV"][1], 3),
        "PPV_CI_high": round(m_te["PPV"][2], 3),
        "NPV_mean": round(m_te["NPV"][0], 3),
        "NPV_CI_low": round(m_te["NPV"][1], 3),
        "NPV_CI_high": round(m_te["NPV"][2], 3),
        "Calib_intercept": round(cal_te[0], 3),
        "Calib_slope": round(cal_te[1], 3),
        "Brier": round(cal_te[2], 3),
        "TP": int(tp), "FN": int(fn), "FP": int(fp), "TN": int(tn),
    }
    return out


# ---- run both scenarios ----
if "df_all" not in globals():
    raise RuntimeError("df_all not found. Please run the CSV read cell first.")

if "lgbm_params" not in globals():
    raise RuntimeError("Cannot find lgbm_params. Please run the main analysis cell (including tune_lgbm) first.")

thr_temporal = globals().get("thr_use", FIXED_THRESHOLD)

temporal_rows = []

row1 = run_temporal_holdout_lgbm(
    df_raw=df_all,
    train_years=[2018, 2019],
    test_years=[2020, 2021],
    scenario_name="Temporal hold-out: Train 2018-2019 -> Test 2020-2021",
    params_base=lgbm_params,
    thr=thr_temporal,
    date_col_override=None,
)
if row1 is not None:
    temporal_rows.append(row1)

row2 = run_temporal_holdout_lgbm(
    df_raw=df_all,
    train_years=[2018, 2019, 2020, 2021],
    test_years=[2022, 2023, 2024],
    scenario_name="Temporal hold-out: Train 2018-2021 -> Test 2022-2024",
    params_base=lgbm_params,
    thr=thr_temporal,
    date_col_override=None,
)
if row2 is not None:
    temporal_rows.append(row2)

temporal_df = pd.DataFrame(temporal_rows)
display(temporal_df)


# ===== Sensitivity analysis: EPV ≈ 10 penalized logistic regression =====
# Aim:
#  1) Determine k such that EPV ≈ 10 using the number of events in the development (training) set
#  2) Rank predictors by stability selection frequency and select the top-k predictors
#  3) Train a penalized logistic regression model on the selected predictors
#  4) Report AUROC with bootstrap 95% CI on the independent test set
#  5) Additionally, derive an operating threshold on OOF predictions (FNR<=5%, FPR<=50%),
#     and report test-set FNR/FPR (with bootstrap 95% CI)

TARGET_EPV = 10.0

n_events_dev = int(y_train.sum())
k_epv10 = int(np.floor(n_events_dev / TARGET_EPV))
k_epv10 = max(1, k_epv10)

print("\n[EPV≈10 LR] target EPV", TARGET_EPV)
print("[EPV≈10 LR] events in development set", n_events_dev)
print("[EPV≈10 LR] k predictors", k_epv10, "EPV", round(n_events_dev / k_epv10, 2))

# Prepare a feature ranking by stability selection frequency.

base_cv_ss2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
_, freq_tmp = stability_select_features(
    x_train, y_train,
    R=100,
    subsample=0.5,
    threshold=0.0,
    base_cv=base_cv_ss2,
    l1_ratio=None,
    seed=SEED
)
rank_df = pd.DataFrame(
    {"feature": list(freq_tmp.keys()), "selection_freq": list(freq_tmp.values())}
).sort_values("selection_freq", ascending=False).reset_index(drop=True)

feats_epv10 = rank_df["feature"].iloc[:k_epv10].tolist()

print("[EPV≈10 LR] selected predictors")
print(feats_epv10)

X_tr_epv10 = x_train[feats_epv10].copy()
X_te_epv10 = x_test[feats_epv10].copy()

def tune_fit_logreg_penalized_only(x_train_local, y_train_local, cv_splits_local, n_trials=100):
    def make_lr(params):
        C = params["C"]
        penalty = params["penalty"]
        if penalty == "elasticnet":
            return LogisticRegression(
                C=C, penalty="elasticnet", l1_ratio=params["l1_ratio"],
                solver="saga", max_iter=2000, random_state=SEED
            )
        if penalty == "l1":
            return LogisticRegression(
                C=C, penalty="l1", solver="liblinear",
                max_iter=2000, random_state=SEED
            )
        return LogisticRegression(
            C=C, penalty="l2", solver="lbfgs",
            max_iter=2000, random_state=SEED
        )

    def objective(trial):
        C = trial.suggest_float("C", 1e-6, 1e3, log=True)
        penalty = trial.suggest_categorical("penalty", ["l1", "l2", "elasticnet"])
        params = {"C": C, "penalty": penalty}
        if penalty == "elasticnet":
            params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)
        model = make_lr(params)
        scores = cross_val_score(
            model, x_train_local, y_train_local,
            cv=list(cv_splits_local),
            scoring="roc_auc"
        )
        return float(np.mean(scores))

    study = optuna.create_study(direction="maximize", sampler=optuna.samplers.TPESampler(seed=SEED))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
    best = study.best_params
    if "l1_ratio" not in best:
        best["l1_ratio"] = None
    model = make_lr(best)
    model.fit(x_train_local, y_train_local)
    return model

# Fit the penalized LR model on the selected predictors
lr_epv10 = tune_fit_logreg_penalized_only(X_tr_epv10, y_train, cv_splits_full, n_trials=100)
print("[EPV≈10 LR] fitted model penalty", lr_epv10.penalty, "C", lr_epv10.C)

# Build OOF predictions and fold-averaged test predictions (for threshold selection on OOF)
lr_params = lr_epv10.get_params()
keep_keys = ["C", "penalty", "solver", "l1_ratio", "max_iter", "random_state"]
lr_params = {k: lr_params[k] for k in keep_keys if k in lr_params}

oof_epv10 = np.zeros(len(y_train), dtype=float)
test_folds_epv10 = np.zeros((len(y_test), len(cv_splits_full)), dtype=float)

for i, (tr_idx, va_idx) in enumerate(cv_splits_full):
    m = LogisticRegression(**lr_params)
    m.fit(X_tr_epv10.iloc[tr_idx], y_train.iloc[tr_idx])
    oof_epv10[va_idx] = m.predict_proba(X_tr_epv10.iloc[va_idx])[:, 1]
    test_folds_epv10[:, i] = m.predict_proba(X_te_epv10)[:, 1]

te_epv10 = test_folds_epv10.mean(axis=1)

# AUROC and bootstrap 95% CI
idx_cv_epv10 = stratified_bootstrap_indices(y_train, n_boot=5000, seed=SEED)
idx_te_epv10 = stratified_bootstrap_indices(y_test, n_boot=5000, seed=SEED)

auc_oof, lo_oof, hi_oof = auc_ci_boot(y_train, oof_epv10, idx_cv_epv10)
auc_te, lo_te, hi_te = auc_ci_boot(y_test, te_epv10, idx_te_epv10)

print("[EPV≈10 LR] OOF AUROC", round(auc_oof, 3), "CI", round(lo_oof, 3), round(hi_oof, 3))
print("[EPV≈10 LR] Test AUROC", round(auc_te, 3), "CI", round(lo_te, 3), round(hi_te, 3))

cal_int, cal_slope, brier = calibration_stats(y_test, te_epv10)
print("[EPV≈10 LR] Calibration intercept", round(cal_int, 3),
      "slope", round(cal_slope, 3),
      "brier", round(brier, 3))

# Choose operating threshold on OOF, then apply to test
thr_epv10 = pick_threshold_constrained(y_train, oof_epv10, fnr_max=0.05, fpr_max=0.50, prefer="min_fpr")
print("[EPV≈10 LR] OOF-derived operating threshold", round(thr_epv10, 3))

m_te_epv10 = metrics_ci_boot(y_test, te_epv10, thr_epv10, idx_te_epv10)

sens_m, sens_lo, sens_hi = m_te_epv10["sensitivity"]
spec_m, spec_lo, spec_hi = m_te_epv10["specificity"]

fnr_m = 1 - sens_m
fnr_lo = 1 - sens_hi
fnr_hi = 1 - sens_lo

fpr_m = 1 - spec_m
fpr_lo = 1 - spec_hi
fpr_hi = 1 - spec_lo

print("[EPV≈10 LR] Test FNR", round(fnr_m, 3), "CI", round(fnr_lo, 3), round(fnr_hi, 3))
print("[EPV≈10 LR] Test FPR", round(fpr_m, 3), "CI", round(fpr_lo, 3), round(fpr_hi, 3))

sens, spec, ppv, npv, tn, fp, fn, tp = metrics_at_threshold(y_test, te_epv10, thr_epv10)
print("[EPV≈10 LR] Confusion matrix TP", tp, "FN", fn, "FP", fp, "TN", tn)

# Optional: compare with an existing full LR model if available
if "te_lr" in globals():
    res = delong_test_correlated(y_test, te_lr, te_epv10)
    print("[DeLong] full LR minus EPV≈10 LR delta AUC", round(res["delta"], 3), "p", round(res["p"], 3))